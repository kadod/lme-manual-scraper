#!/usr/bin/env python3
"""
L Message Manual URL Scraper
https://lme.jp/manual/ ã‹ã‚‰ã™ã¹ã¦ã®URLãƒ‘ã‚¹ã‚’å–å¾—ã—ã¦æ•´ç†ã—ã¾ã™
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
from collections import defaultdict
import time

class LmeManualScraper:
    def __init__(self, base_url="https://lme.jp/manual/"):
        self.base_url = base_url
        self.visited_urls = set()
        self.all_paths = []
        self.categorized_paths = defaultdict(list)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })

    def is_valid_url(self, url):
        """URLãŒæœ‰åŠ¹ã§lme.jp/manual/é…ä¸‹ã‹ãƒã‚§ãƒƒã‚¯"""
        parsed = urlparse(url)
        return (
            parsed.netloc == 'lme.jp' and
            parsed.path.startswith('/manual/') and
            not parsed.path.endswith(('.jpg', '.png', '.gif', '.css', '.js', '.pdf'))
        )

    def fetch_page(self, url):
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return None

    def extract_links(self, html, current_url):
        """HTMLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'html.parser')
        links = set()

        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urljoin(current_url, href)

            # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆ(#)ã‚’é™¤å»
            full_url = full_url.split('#')[0]

            if self.is_valid_url(full_url):
                links.add(full_url)

        return links

    def categorize_path(self, url):
        """URLã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        path = urlparse(url).path
        parts = [p for p in path.split('/') if p]

        if len(parts) <= 1:
            return 'ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸'
        elif 'category' in parts:
            return 'ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸'
        elif 'tutorial' in path:
            return 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«'
        elif 'learn-video' in path:
            return 'å‹•ç”»å­¦ç¿’'
        elif 'contact' in path:
            return 'ãŠå•ã„åˆã‚ã›'
        elif 'information' in path or 'update' in path:
            return 'ãŠçŸ¥ã‚‰ã›ãƒ»ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ'
        elif 'interview' in path or 'introduction_example' in path:
            return 'å°å…¥äº‹ä¾‹'
        else:
            return 'è¨˜äº‹ãƒ»ãƒãƒ‹ãƒ¥ã‚¢ãƒ«'

    def scrape(self, max_depth=2):
        """å†å¸°çš„ã«ã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«"""
        to_visit = [(self.base_url, 0)]

        while to_visit:
            current_url, depth = to_visit.pop(0)

            if current_url in self.visited_urls or depth > max_depth:
                continue

            print(f"ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {current_url} (æ·±ã•: {depth})")
            self.visited_urls.add(current_url)

            html = self.fetch_page(current_url)
            if not html:
                continue

            # ãƒ‘ã‚¹æƒ…å ±ã‚’ä¿å­˜
            path = urlparse(current_url).path
            self.all_paths.append({
                'url': current_url,
                'path': path,
                'depth': depth
            })

            # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
            category = self.categorize_path(current_url)
            self.categorized_paths[category].append({
                'url': current_url,
                'path': path
            })

            # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
            links = self.extract_links(html, current_url)
            for link in links:
                if link not in self.visited_urls:
                    to_visit.append((link, depth + 1))

            # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å¾…æ©Ÿ
            time.sleep(0.5)

    def save_results(self):
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        # JSONå½¢å¼ã§ä¿å­˜
        with open('lme_manual_paths.json', 'w', encoding='utf-8') as f:
            json.dump({
                'total_pages': len(self.all_paths),
                'all_paths': self.all_paths,
                'categorized': dict(self.categorized_paths)
            }, f, ensure_ascii=False, indent=2)

        # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ä¿å­˜ï¼ˆèª­ã¿ã‚„ã™ã„ç‰ˆï¼‰
        with open('lme_manual_paths.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("L Message ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚µã‚¤ãƒˆ - å…¨URLãƒ‘ã‚¹ä¸€è¦§\n")
            f.write(f"ç·ãƒšãƒ¼ã‚¸æ•°: {len(self.all_paths)}\n")
            f.write("=" * 80 + "\n\n")

            for category, paths in sorted(self.categorized_paths.items()):
                f.write(f"\nã€{category}ã€‘ ({len(paths)}ãƒšãƒ¼ã‚¸)\n")
                f.write("-" * 80 + "\n")
                for item in sorted(paths, key=lambda x: x['path']):
                    f.write(f"  â€¢ {item['path']}\n")
                    f.write(f"    {item['url']}\n\n")

        # Markdownå½¢å¼ã§ä¿å­˜
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write("# L Message ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚µã‚¤ãƒˆ URLä¸€è¦§\n\n")
            f.write(f"**ç·ãƒšãƒ¼ã‚¸æ•°:** {len(self.all_paths)}\n\n")
            f.write("ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ https://lme.jp/manual/ ã®ã™ã¹ã¦ã®URLãƒ‘ã‚¹ã‚’å–å¾—ãƒ»æ•´ç†ã—ãŸã‚‚ã®ã§ã™ã€‚\n\n")

            f.write("## ã‚«ãƒ†ã‚´ãƒªåˆ¥URLä¸€è¦§\n\n")
            for category, paths in sorted(self.categorized_paths.items()):
                f.write(f"### {category} ({len(paths)}ãƒšãƒ¼ã‚¸)\n\n")
                for item in sorted(paths, key=lambda x: x['path']):
                    f.write(f"- [{item['path']}]({item['url']})\n")
                f.write("\n")

        print(f"\nâœ… çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:")
        print(f"  - lme_manual_paths.json (JSONå½¢å¼)")
        print(f"  - lme_manual_paths.txt (ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)")
        print(f"  - README.md (Markdownå½¢å¼)")
        print(f"\nğŸ“Š çµ±è¨ˆ:")
        print(f"  ç·ãƒšãƒ¼ã‚¸æ•°: {len(self.all_paths)}")
        for category, paths in sorted(self.categorized_paths.items()):
            print(f"  {category}: {len(paths)}ãƒšãƒ¼ã‚¸")

def main():
    print("=" * 80)
    print("L Message ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚µã‚¤ãƒˆ URLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼")
    print("=" * 80)
    print()

    scraper = LmeManualScraper()
    scraper.scrape(max_depth=2)
    scraper.save_results()

    print("\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")

if __name__ == "__main__":
    main()
